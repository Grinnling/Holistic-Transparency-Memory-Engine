# =============================================================================
# CODE_IMPLEMENTATION Environment Configuration
# =============================================================================
# This file documents all configurable services in the system.
# Copy to .env and modify as needed. All values have sensible localhost defaults.
#
# For Claude/AI: This is the system topology map. Read this to understand
# what services exist and how they connect.
#
# =============================================================================

# -----------------------------------------------------------------------------
# FRONTEND (React/Vite)
# -----------------------------------------------------------------------------
# Variables prefixed with VITE_ are exposed to the browser.
# These are the endpoints the React app talks to.

# Main API server - handles chat, memory queries, service management
VITE_API_URL=http://localhost:8000

# WebSocket for real-time chat updates
VITE_WS_CHAT_URL=ws://localhost:8000/ws

# WebSocket for visibility/event stream (separate from chat for resilience)
VITE_WS_EVENTS_URL=ws://localhost:8000/ws/events

# WebSocket for terminal streaming (PTY output)
VITE_TERMINAL_WS_URL=ws://localhost:8765

# -----------------------------------------------------------------------------
# MEMORY SYSTEM SERVICES (Backend)
# -----------------------------------------------------------------------------
# These are internal services that the Python backend talks to.
# Not exposed to browser (no VITE_ prefix).

# Working Memory - short-term conversation context
# Handles: current conversation buffer, context window management
WORKING_MEMORY_URL=http://localhost:5001

# Curator - memory curation and relevance scoring
# Handles: deciding what memories to keep, importance ranking
CURATOR_URL=http://localhost:8004

# MCP Logger - logging for MCP (Model Context Protocol) operations
# Handles: tracking tool usage, external service calls
MCP_LOGGER_URL=http://localhost:8001

# Episodic Memory - long-term memory storage and retrieval
# Handles: vector storage, semantic search, memory persistence
EPISODIC_MEMORY_URL=http://localhost:8005

# -----------------------------------------------------------------------------
# LLM BACKENDS
# -----------------------------------------------------------------------------
# Configure which LLM backend to use and its URL.
# Only one is typically active at a time.

# Which backend to use: ollama, lmstudio, textgen
LLM_BACKEND=ollama

# Ollama - local LLM runner (default)
OLLAMA_URL=http://localhost:11434

# LM Studio - local LLM with OpenAI-compatible API
LMSTUDIO_URL=http://localhost:1234

# Text Generation WebUI - Gradio-based local LLM
TEXTGEN_URL=http://localhost:8080

# -----------------------------------------------------------------------------
# LLM MODEL ROLE CONFIGURATION
# -----------------------------------------------------------------------------
# Explicitly map model names to roles for the admin panel status display.
# These help the /llm/status endpoint identify which loaded model serves which role.
# If not set, the system falls back to pattern matching (looking for 'embed', 'rerank', etc.)
# Use partial matches - e.g., "qwen" will match "qwen2.5-coder-7b-instruct"

# Chat/completion model (your main conversational LLM)
LLM_CHAT_MODEL=qwen

# Embedding model (for semantic search)
LLM_EMBEDDING_MODEL=bge-m3

# Rerank model (for result re-ranking, optional)
LLM_RERANK_MODEL=

# -----------------------------------------------------------------------------
# OPTIONAL SERVICES
# -----------------------------------------------------------------------------

# Redis - used for context registry caching (optional)
REDIS_URL=redis://localhost:6379/0

# -----------------------------------------------------------------------------
# DEVELOPMENT SETTINGS
# -----------------------------------------------------------------------------

# React dev server port (Vite default)
VITE_PORT=3000

# API server port
API_PORT=8000

# Terminal broadcaster port
TERMINAL_PORT=8765
